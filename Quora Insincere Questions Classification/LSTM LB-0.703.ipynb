{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d5f1b9360b808d5941d6f37ba2ba20cf3d7f869"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "6a1856073e217798a8c5ddd17107a8a72855b665"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard imports\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# imports for preprocessing the questions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# progress bars\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"F-score is ill-defined and being set to 0.0 due to no predicted samples.\")\n",
    "%matplotlib inline\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9898133bb6210b97bead2ed9c46e8280defb305e"
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "eaa71cd7a40cf4f647b3af9a5b0fdcd903fefeee"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "11e583156d0019296a72acf84158a1de8cb18ab0"
   },
   "outputs": [],
   "source": [
    "enable_local_test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "68dcf13011432b865b124ca0b4e8262b9516bc70"
   },
   "outputs": [],
   "source": [
    "if enable_local_test:\n",
    "    n_test = len(test_df) * 4 #225480\n",
    "    train_df, local_test_df = (train_df.iloc[:-n_test].reset_index(drop=True), \n",
    "                               train_df.iloc[-n_test:].reset_index(drop=True)) #ケツから-n_testインデックス\n",
    "else:\n",
    "    local_test_df = pd.DataFrame([[None, None, 0], [None, None, 0]], columns=['qid', 'question_text', 'target'])\n",
    "    n_test = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "6466a072849ff0f6df9168786265a50f793fc5b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_test = 225480\n",
      "train_df rows: 1306122\n",
      "local_test_df rows: 2\n",
      "ture test_df rows: 56370\n"
     ]
    }
   ],
   "source": [
    "print('n_test = {}'.format(len(test_df)*4))\n",
    "print('train_df rows: {}'.format(len(train_df)))\n",
    "print('local_test_df rows: {}'.format(len(local_test_df)))\n",
    "print('ture test_df rows: {}'.format(len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "301805519ef9c9a7d59eb6c6945480d824d6e160"
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "d35808558a9a2d4288ff7993c76f542e87edd62f"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "6c91fec794f4f77aa09a69ca5eee3e7314a99f33"
   },
   "outputs": [],
   "source": [
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)], disable=True):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "4ea13f2e32ffa7c050af2c2dd9423ed2aa2592f0"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93f521adfd488bc3472b3184a3dcff92da0eb3c9"
   },
   "source": [
    "# Processing input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ae76151d67efb81627b03454d7bf8f692ce15715"
   },
   "source": [
    "- embed_sizeは300でいいと思う\n",
    "- max_wordsを絞るとスコアが下がる気がするんだよね。\n",
    "- maxlen=80だとNG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "9a4874c7936cd1e527a82aa779e24fc3de3e23a1"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "max_features= 120000 #200000 #120000 #95000 #200000\n",
    "maxlen = 70\n",
    "filters = '\\t\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "d98f33c48cabfb0f42b4fbda343170206802dd46"
   },
   "outputs": [],
   "source": [
    "puncts = (',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\",\n",
    "          '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', '·', \n",
    "          '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', \n",
    "          '§', '″', '′', 'Â', '█', '½', '…', '“', '★', '”', '–', '●', '►', '−', '¢', \n",
    "          '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    "          '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', '¯', '♦', \n",
    "          '¤', '▲', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '♪',\n",
    "          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√',\n",
    "          '✔','，','？','℃','＞','！','。','／','；','„','‛','₹','÷','θ','π','Σ','Δ','ʃ','≠',\n",
    "          '∈', '≡', '＝', 'Σ', 'Δ','∘','℅','≥','◦','ºF','∫','∠', '∑','∇','✓','∆',\n",
    "          '\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', \n",
    "          '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0',\n",
    "         )\n",
    "def clean_punct(x, puncts):\n",
    "    if x is not None:\n",
    "        for punct in puncts:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "85c8ec531f81e53fc75127412350e6f3b2dea319"
   },
   "outputs": [],
   "source": [
    "train_question_row = train_df.copy()\n",
    "test_question_row = test_df.copy()\n",
    "local_question_row = local_test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "f6c40d17110f91c7d0581ea7a2bf9f264a3e41be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 53s, sys: 928 ms, total: 1min 54s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for df in [train_df, test_df, local_test_df]:\n",
    "    df[\"question_text\"] = df[\"question_text\"].str.lower()\n",
    "    #df[\"question_text\"] = df[\"question_text\"].map(replace_typical_misspell)\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: clean_punct(x, puncts))\n",
    "    df[\"question_text\"].fillna(\"_##_\", inplace=True)\n",
    "    \n",
    "x_train = train_df[\"question_text\"].values\n",
    "x_test = test_df[\"question_text\"].values\n",
    "x_test_local = local_test_df[\"question_text\"].values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, filters=filters)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test_local)+list(x_test))\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_local = tokenizer.texts_to_sequences(x_test_local)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "x_test_local = pad_sequences(x_test_local, maxlen=maxlen)\n",
    "\n",
    "y_train = train_df['target'].values\n",
    "y_test = local_test_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "676a9514e49d8438d56c338354f1805ad490bb8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train rows 1306122\n",
      "y_train rows 1306122\n",
      "x_test_local rows 2\n",
      "y_test rows 2\n"
     ]
    }
   ],
   "source": [
    "print('x_train rows {}'.format(len(x_train)))\n",
    "print('y_train rows {}'.format(len(y_train)))\n",
    "print('x_test_local rows {}'.format(len(x_test_local)))\n",
    "print('y_test rows {}'.format(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8607fe327a4d7f2af0dfad02cb7fae0a14b0f2b4"
   },
   "source": [
    "# Creating the embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "e5df8649c4db0a6e5bbf05088614c8cde4985081"
   },
   "outputs": [],
   "source": [
    "## FUNCTIONS TAKEN FROM https://www.kaggle.com/gmhost/gru-capsule\n",
    "def load_glove(word_index, embeddings_index=None):\n",
    "    if not embeddings_index:\n",
    "        EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "        def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    \n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index)+1)\n",
    "    word_in_embed = []\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            word_in_embed.append(word)\n",
    "            \n",
    "    return embedding_matrix, word_in_embed\n",
    "\n",
    "def load_para(word_index, embeddings_index=None):\n",
    "    if not embeddings_index:\n",
    "        EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "        def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "    \n",
    "    emb_mean,emb_std = -0.0053247833,0.49346462\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    word_in_embed = []\n",
    "    nb_words = min(max_features, len(word_index)+1)\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            word_in_embed.append(word)\n",
    "    \n",
    "    return embedding_matrix, word_in_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "bb4c2db45def7cd4ed9880c05281cc789306deff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 33s, sys: 9.77 s, total: 5min 43s\n",
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_in_glove = []\n",
    "word_in_para = []\n",
    "\n",
    "embeddings_index_glove, word_in_glove = load_glove(tokenizer.word_index)\n",
    "embeddings_index_para, word_in_para = load_para(tokenizer.word_index)\n",
    "\n",
    "embedding_matrix = np.mean([embeddings_index_glove, embeddings_index_para], axis=0)\n",
    "del embeddings_index_glove ,embeddings_index_para\n",
    "gc.collect()\n",
    "\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "939247ba56db3e629618ea51d98aaf43ead8840e"
   },
   "outputs": [],
   "source": [
    "def make_addtional_features(token, word_in_embed):\n",
    "    '''\n",
    "    chars - 文字数\n",
    "    words - 単語数\n",
    "    words_vs_unique - テキストに含まれる語彙の中でユニークなワードの割合\n",
    "    unknown_count - embedにない語彙(unknown)の数\n",
    "    max_len_unknown - unknownな語彙の最大長さ\n",
    "    min_len_unknown - unknownな語彙の最小長さ 　あまり効果ないと思うので外した\n",
    "    unknown_vs_words - テキスト含まれる語彙の中でunknownなワードの割合\n",
    "    last_question - 最後の単語がクエスチョンかどうか\n",
    "    captial_rate - isupperの単語数 / 単語数\n",
    "    '''\n",
    "    features = []\n",
    "    for sentence in tqdm(token):\n",
    "        isupper = sum(1 for s in sentence if s.isupper()) #uppercount\n",
    "        sentence = [w.lower() for w in sentence] #小文字化\n",
    "        chars = len(''.join(sentence))\n",
    "        words = len(sentence)\n",
    "        unique_word = len(set(sentence))\n",
    "        \n",
    "        last_question = 0\n",
    "        if token[len(sentence)-1] == '?':\n",
    "            last_question = 1\n",
    "        \n",
    "        \n",
    "        words_vs_unique = 0\n",
    "        captial_rate = 0\n",
    "        \n",
    "        if words != 0:\n",
    "            words_vs_unique = (unique_word/words)\n",
    "            captial_rate = (isupper/words)\n",
    "\n",
    "        unknown_count = 0\n",
    "        max_len_unknown = 0\n",
    "        min_len_unknown = 0\n",
    "        unknown_vs_words = 0\n",
    "        ##ここからワードのチェック##\n",
    "        for word in sentence:\n",
    "            if not word in word_in_embed:\n",
    "                unknown_count +=1\n",
    "                if max_len_unknown < len(word):\n",
    "                    max_len_unknown = len(word)\n",
    "                if min_len_unknown == 0:\n",
    "                    min_len_unknown = len(word)\n",
    "                elif min_len_unknown > len(word):\n",
    "                    min_len_unknown = len(word)\n",
    "        \n",
    "        if unknown_count !=0 :\n",
    "            unknown_vs_words = unknown_count / words\n",
    "\n",
    "        features.append([max_len_unknown, captial_rate])\n",
    "    \n",
    "    features = np.array(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "b4c55a6e3ac7dad6e8755b03823762de0a9b78fd"
   },
   "outputs": [],
   "source": [
    "word_in_embed = set(word_in_glove+word_in_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "9a6c9476d9d7126b35d0d41d70996dbbb9aa0fe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 996 ms, total: 1min 13s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Captialの情報を失いたくないのでここでlowerをしないでトークン化を行う\n",
    "for df in [train_question_row, test_question_row, local_question_row]:\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: clean_punct(x, puncts))\n",
    "    df.fillna(\"_##_\", inplace=True)\n",
    "\n",
    "train_text_token = [text_to_word_sequence(text, lower=False, filters=filters) for text in train_question_row['question_text'].values]\n",
    "test_text_token = [text_to_word_sequence(text, lower=False, filters=filters) for text in test_question_row['question_text'].values]\n",
    "local_test_token = [text_to_word_sequence(text, lower=False, filters=filters) for text in local_question_row['question_text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "1cf6826b3bf648bdc67e1ee861e68214700b04cd"
   },
   "outputs": [],
   "source": [
    "# train_text_token = [text_to_word_sequence(text) for text in train_df['question_text'].values]\n",
    "# test_text_token = [text_to_word_sequence(text) for text in test_df['question_text'].values]\n",
    "# local_test_token = [text_to_word_sequence(text) for text in local_test_df['question_text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "b2717f868d06a55f897b075361dd3149d956c752"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:14<00:00, 93162.70it/s] \n",
      "100%|██████████| 56370/56370 [00:00<00:00, 110998.66it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2778.60it/s]\n"
     ]
    }
   ],
   "source": [
    "features = make_addtional_features(train_text_token,word_in_embed)\n",
    "test_features = make_addtional_features(test_text_token,word_in_embed)\n",
    "test_local_features =  make_addtional_features(local_test_token,word_in_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "d2880aa687cc72180bcbd025e952e57f1c5ce5f6"
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(features)\n",
    "features = ss.transform(features)\n",
    "test_features = ss.transform(test_features)\n",
    "test_local_features = ss.transform(test_local_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "b684da83787284473130e898c659e0964844eddc"
   },
   "outputs": [],
   "source": [
    "y_train = train_df['target'].values\n",
    "y_test = local_test_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "1c41c927435ab7ef6cc82e1a030b040bd8b8788a"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1029)\n",
    "trn_idx = np.random.permutation(len(x_train))\n",
    "\n",
    "x_train = x_train[trn_idx]\n",
    "y_train = y_train[trn_idx]\n",
    "features = features[trn_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "02006fdd173c8fd5ec1034dc60beba52367574e4"
   },
   "outputs": [],
   "source": [
    "np.save(\"x_train\",x_train)\n",
    "np.save(\"x_test\",x_test)\n",
    "np.save(\"y_train\",y_train)\n",
    "\n",
    "np.save(\"features\",features)\n",
    "np.save(\"test_features\",test_features)\n",
    "np.save('test_local_features', test_local_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "077bd1f1eb42735ca678b6699edef99c414303c6"
   },
   "outputs": [],
   "source": [
    "x_train = np.load(\"x_train.npy\")\n",
    "x_test = np.load(\"x_test.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "features = np.load(\"features.npy\")\n",
    "test_features = np.load(\"test_features.npy\")\n",
    "test_local_features = np.load('test_local_features.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd3804f9bf44762f7715e23099d42f606a4c281b"
   },
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "3075b2fcadb289ca04de19a7edc0977e7392b2dd"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss dosen't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=0, verbose=False,filename='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 0\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.filename = filename\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.filename )\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "a3e084b4fe81baf1bc392d36e2d43b2f6e032331"
   },
   "outputs": [],
   "source": [
    "n_splits = 4\n",
    "splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1024).split(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "deeefefacaa5c95cee63ea2b24a1affef82a9d9f"
   },
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    \"\"\"Gaussian noise regularizer.\n",
    "\n",
    "    Args:\n",
    "        sigma (float, optional): relative standard deviation used to generate the\n",
    "            noise. Relative means that it will be multiplied by the magnitude of\n",
    "            the value your are adding the noise to. This means that sigma can be\n",
    "            the same regardless of the scale of the vector.\n",
    "        is_relative_detach (bool, optional): whether to detach the variable before\n",
    "            computing the scale of the noise. If `False` then the scale of the noise\n",
    "            won't be seen as a constant but something to optimize: this will bias the\n",
    "            network to generate vectors with smaller values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma=0.1, is_relative_detach=True):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "        self.is_relative_detach = is_relative_detach\n",
    "        self.noise = torch.tensor(0 ,dtype=torch.float).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.sigma != 0:\n",
    "            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n",
    "            sampled_noise = self.noise.repeat(*x.size()).normal_() * scale\n",
    "            x = x + sampled_noise\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "c78fe9f2abf0db1b3aaf39b97d9303bf03c24d18"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "313b59ffab2f076409a4bd4e680e3b49e9c923a3"
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        hidden_size = 90\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding_weight = torch.FloatTensor(embedding_matrix)\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.embedding_weight, freeze=True)\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm_attention = Attention(hidden_size * 2, maxlen)\n",
    "        self.gru_attention = Attention(hidden_size * 2, maxlen)\n",
    "        \n",
    "        #self.batchnorm = nn.BatchNorm1d(hidden_size*2*4)\n",
    "        self.gaussian = GaussianNoise(0.1)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size*2*4+2, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x[0])\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "        #hiddenの初期状態\n",
    "        #batch_size = x[0].shape[0]\n",
    "        #h0 = nn.Parameter(torch.randn(2, batch_size, self.hidden_size).type(torch.float32), requires_grad=False).cuda() #ランダムに初期化\n",
    "        #c0 = nn.Parameter(torch.randn(2, batch_size, self.hidden_size).type(torch.float32), requires_grad=False).cuda()\n",
    "        #h0 = torch.zeros(2, batch_size,  self.hidden_size).cuda()\n",
    "        #c0 = torch.zeros(2, batch_size,  self.hidden_size).cuda()\n",
    "        \n",
    "        #h_lstm, _ = self.lstm(h_embedding,(h0, c0))\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "        \n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        \n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        \n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool), 1)\n",
    "        #conc = self.batchnorm(conc)\n",
    "        conc = self.gaussian(conc)\n",
    "        \n",
    "        f = torch.tensor(x[1], dtype=torch.float).cuda()\n",
    "        \n",
    "        conc = torch.cat((conc, f), 1)\n",
    "        \n",
    "        #conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool, f), 1)\n",
    "        #conc = self.batchnorm(conc)\n",
    "        #conc = self.gaussian(conc)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a5445e83f96ca47abded343f42ed702ad5ecaca7"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "49ab78365628601cece06c57a426e2512061966d"
   },
   "source": [
    "Regarding the training procedure, we use Cyclic LR with 5 epochs. I also made a separate function (`train_model`) to train the model because we are going to use it multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "51453ae35fce480b307c64c4e8a82828e785001f"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "n_epochs = 5\n",
    "clip = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "1ff4129c342ea6f6de6fd35377a9d426798409d3"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, factor=0.6, min_lr=1e-4, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, torch.optim.Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "        \n",
    "        self.last_loss = np.inf\n",
    "        self.min_lr = min_lr\n",
    "        self.factor = factor\n",
    "        \n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def step(self, loss):\n",
    "        if loss > self.last_loss:\n",
    "            self.base_lrs = [max(lr * self.factor, self.min_lr) for lr in self.base_lrs]\n",
    "            self.max_lrs = [max(lr * self.factor, self.min_lr) for lr in self.max_lrs]\n",
    "            \n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "3e1cf9e73829e37097f85ab92bbddf9d40e43d1e"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "\n",
    "        return data, target, index\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "d82dd938856bfa6cdcae3d02b8c79a4923125d3f"
   },
   "outputs": [],
   "source": [
    "x_test_cuda = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "160695238314f1b600e510df47a24c22c36b717e"
   },
   "outputs": [],
   "source": [
    "x_test_cuda = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "x_test_local_cuda = torch.tensor(x_test_local, dtype=torch.long).cuda()\n",
    "test_local = torch.utils.data.TensorDataset(x_test_local_cuda)\n",
    "test_local_loader = torch.utils.data.DataLoader(test_local, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "e3f8c489120aee31f6f00fcb6e1b11036f427fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/5 \t loss=65.6226 \t val_loss=51.6962 \t val_f1=0.6737 best_t=0.36 \t time=250.88s\n",
      "Epoch 2/5 \t loss=57.6233 \t val_loss=50.6934 \t val_f1=0.6826 best_t=0.31 \t time=252.08s\n",
      "Epoch 3/5 \t loss=54.1142 \t val_loss=50.2969 \t val_f1=0.6855 best_t=0.39 \t time=251.45s\n",
      "Epoch 4/5 \t loss=50.4237 \t val_loss=51.2529 \t val_f1=0.6851 best_t=0.34 \t time=251.30s\n",
      "Epoch 5/5 \t loss=46.6014 \t val_loss=52.5747 \t val_f1=0.6827 best_t=0.30 \t time=252.04s\n",
      "Validation loss:  52.574651120224736\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/5 \t loss=64.8794 \t val_loss=55.9611 \t val_f1=0.6683 best_t=0.18 \t time=251.63s\n",
      "Epoch 2/5 \t loss=57.0850 \t val_loss=50.3772 \t val_f1=0.6828 best_t=0.37 \t time=252.07s\n",
      "Epoch 3/5 \t loss=53.5185 \t val_loss=49.8198 \t val_f1=0.6823 best_t=0.39 \t time=252.68s\n",
      "Epoch 4/5 \t loss=49.7282 \t val_loss=49.9711 \t val_f1=0.6866 best_t=0.32 \t time=251.75s\n",
      "Epoch 5/5 \t loss=45.7953 \t val_loss=52.2739 \t val_f1=0.6846 best_t=0.32 \t time=252.69s\n",
      "Validation loss:  52.273865786465755\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/5 \t loss=65.9894 \t val_loss=52.7976 \t val_f1=0.6679 best_t=0.25 \t time=252.46s\n",
      "Epoch 2/5 \t loss=57.7408 \t val_loss=50.8224 \t val_f1=0.6778 best_t=0.36 \t time=252.16s\n",
      "Epoch 3/5 \t loss=54.0331 \t val_loss=50.9059 \t val_f1=0.6837 best_t=0.28 \t time=252.01s\n",
      "Epoch 4/5 \t loss=50.8030 \t val_loss=51.1759 \t val_f1=0.6825 best_t=0.32 \t time=252.79s\n",
      "Epoch 5/5 \t loss=47.2919 \t val_loss=52.5814 \t val_f1=0.6820 best_t=0.36 \t time=252.19s\n",
      "Validation loss:  52.581374398593276\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/5 \t loss=65.5594 \t val_loss=51.7851 \t val_f1=0.6739 best_t=0.35 \t time=252.06s\n",
      "Epoch 2/5 \t loss=57.4062 \t val_loss=50.4750 \t val_f1=0.6799 best_t=0.35 \t time=252.29s\n",
      "Epoch 3/5 \t loss=54.0192 \t val_loss=50.5255 \t val_f1=0.6854 best_t=0.44 \t time=253.12s\n",
      "Epoch 4/5 \t loss=50.1457 \t val_loss=50.6289 \t val_f1=0.6877 best_t=0.33 \t time=252.75s\n",
      "Epoch 5/5 \t loss=46.3860 \t val_loss=53.7201 \t val_f1=0.6824 best_t=0.29 \t time=252.78s\n",
      "Validation loss:  53.72010043272777\n",
      "\n",
      "CPU times: user 56min 3s, sys: 29min 15s, total: 1h 25min 19s\n",
      "Wall time: 1h 25min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_preds = np.zeros(len(train_df))\n",
    "test_preds = np.zeros((len(test_df), len(splits)))\n",
    "test_preds_local = np.zeros((n_test, len(splits)))\n",
    "seed = 6017\n",
    "\n",
    "for ii, (train_idx, valid_idx) in enumerate(splits):\n",
    "    fold_stime = time.time()\n",
    "    \n",
    "    x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    kfold_X_features = features[train_idx.astype(int)]\n",
    "    kfold_X_valid_features = features[valid_idx.astype(int)]\n",
    "    x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    seed_everything(seed + ii)\n",
    "    \n",
    "    model = NeuralNet()\n",
    "    model.cuda()\n",
    "\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "    step_size = 300\n",
    "    base_lr, max_lr = 0.001, 0.003   \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n",
    "    \n",
    "    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,step_size=step_size, \n",
    "                         mode='exp_range',gamma=0.99994)\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train = MyDataset(train)\n",
    "    valid = MyDataset(valid)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    #early_stopping = EarlyStopping(patience=0,verbose=True)\n",
    "    print(f'Fold {ii + 1}')\n",
    "    \n",
    "    #traning start\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.  \n",
    "        \n",
    "        for i, (x_batch, y_batch, index) in enumerate(train_loader):\n",
    "            f = kfold_X_features[index]\n",
    "            y_pred = model([x_batch,f])\n",
    "            \n",
    "            if scheduler:\n",
    "                scheduler.batch_step()\n",
    "            \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            \n",
    "            optimizer.step()          \n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        ###1epoch end###\n",
    "        #検証   \n",
    "        model.eval()\n",
    "        \n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros((len(test_df)))\n",
    "        \n",
    "        validate = True\n",
    "        avg_val_loss = 0.\n",
    "\n",
    "        for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "            f = kfold_X_valid_features[index]\n",
    "            y_pred = model([x_batch,f]).detach()\n",
    "\n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "        search_result = threshold_search(y_val_fold.cpu().numpy(), valid_preds_fold)\n",
    "        val_f1, val_threshold = search_result['f1'], search_result['threshold']\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} best_t={:.2f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, n_epochs, avg_loss, avg_val_loss, val_f1, val_threshold, elapsed_time))\n",
    "        \n",
    "#         early_stopping(1.-val_f1, model)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print('Early stopping at',epoch+1,'epoch')\n",
    "#             break\n",
    "        \n",
    "    ####1fold end####\n",
    "    #model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "    \n",
    "    avg_val_loss = 0.\n",
    "    for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "        f = kfold_X_valid_features[i * batch_size:(i+1) * batch_size]\n",
    "        y_pred = model([x_batch, f]).detach()\n",
    "\n",
    "        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    print('Validation loss: ', avg_val_loss)\n",
    "\n",
    "    test_preds_fold = np.zeros((len(test_loader.dataset)))\n",
    "    \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        f = test_features[i * batch_size:(i+1) * batch_size]\n",
    "        y_pred = model([x_batch,f]).detach()\n",
    "\n",
    "        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    test_preds_local_fold = np.zeros((len(test_local_loader.dataset)))\n",
    "    \n",
    "    for i, (x_batch,) in enumerate(test_local_loader):\n",
    "        f = test_local_features[i * batch_size:(i+1) * batch_size]\n",
    "        y_pred = model([x_batch, f]).detach()\n",
    "\n",
    "        test_preds_local_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds[:, ii] = test_preds_fold\n",
    "    test_preds_local[:, ii] = test_preds_local_fold\n",
    "    elapsed_time = time.time() - fold_stime\n",
    "    print()\n",
    "    #break\n",
    "    \n",
    "#print('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f647692ec05df5f637f5cb4b4429037ea6d0001"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "2346d3937a2f82bc4512036315e7d32e5509d150"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.34, 'f1': 0.6822392034271159}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result = threshold_search(y_train, train_preds)\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "67ed4c346322597ffc27ac55864af321cbbb7c72"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3\n",
       "0 NaN NaN NaN NaN\n",
       "1 NaN NaN NaN NaN\n",
       "2 NaN NaN NaN NaN\n",
       "3 NaN NaN NaN NaN"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_preds_local).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "3829e5fc69a3ec57a1a302ac51664ce0bbd56062"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, test_preds_local.mean(axis=1) > search_result['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "e87e9557c51794f9744e01abc27c6ff380422845"
   },
   "outputs": [],
   "source": [
    "submission = test_df[['qid']].copy()\n",
    "submission['prediction'] = test_preds.mean(axis=1) > search_result['threshold']\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
